{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Untr0nix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Untr0nix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd                       \n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import nltk\n",
    "import abc\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def apply(self,obj):\n",
    "        pass\n",
    "\n",
    "class SimpleTokenizer(PreProcessor):\n",
    "    def apply(self,text):\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "class Lemmatization(PreProcessor):\n",
    "    \n",
    "    def __init__(self,lemmatizer):\n",
    "        self._lemmatizer = lemmatizer\n",
    "        self.table = str.maketrans({key: None for key in string.punctuation})\n",
    "    \n",
    "    def apply(self,text):\n",
    "        filtered_tokens = []\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        text = text.translate(self.table)\n",
    "        for word in text.split():\n",
    "            if word not in stop_words:\n",
    "                filtered_tokens.append(self._lemmatizer.lemmatize(word.lower()))\n",
    "        return filtered_tokens\n",
    "\n",
    "class AdvancedLemmatization(PreProcessor):\n",
    "    def __init__(self,lemmatizer):\n",
    "        self._lemmatizer = lemmatizer\n",
    "        self.table = str.maketrans({key: None for key in string.punctuation})\n",
    "    \n",
    "    def apply(self,text):\n",
    "        filtered_tokens = []\n",
    "        lemmatized_tokens = []\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        text = text.translate(self.table)\n",
    "        for word in text.split():\n",
    "            if word not in stop_words:\n",
    "                lemmatized_tokens.append(self._lemmatizer.lemmatize(word.lower()))\n",
    "            filtered_tokens = [' '.join(l) for l in nltk.bigrams(lemmatized_tokens)] + lemmatized_tokens\n",
    "        return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(abc.ABC):\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def encode(self,item):\n",
    "        pass\n",
    "\n",
    "class BinaryEncoder(Encoder):\n",
    "    def __init__(self):\n",
    "        self._features = Counter()\n",
    "    def encode(self,tokens): \n",
    "        self._features += Counter(tokens)\n",
    "        return Counter(tokens)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    \n",
    "    def __init__(self,path,label_column = 'LABEL',delimiter = '\\t',mapping = None,transformators = []):\n",
    "        self._data = pd.read_csv(path,delimiter = delimiter)\n",
    "        if mapping is not None:\n",
    "            self._data[label_column] = self._data[label_column].map(mapping)\n",
    "        self.raw_data = []\n",
    "        self.preprocessed = []\n",
    "        self._transformators = transformators\n",
    "    \n",
    "    def extract_item(self,position,args):\n",
    "        return tuple(self._data[args].iloc[position])\n",
    "    \n",
    "    def transform(self,args,apply_to):\n",
    "        self.index = apply_to\n",
    "        for idx in range(self._data.shape[0]):\n",
    "            items = self.extract_item(idx,args)\n",
    "            self.raw_data.append(items)\n",
    "            transformed = items[apply_to]\n",
    "            for transformer in self._transformators:\n",
    "                transformed = transformer.apply(transformed)\n",
    "            temp_items = list(items)\n",
    "            temp_items[apply_to] = transformed\n",
    "            self.preprocessed.append(tuple(temp_items))\n",
    "\n",
    "\n",
    "            \n",
    "                        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification:\n",
    "    def __init__(self,dataset,model = LinearSVC,**kwargs):\n",
    "        self._dataset = dataset\n",
    "        self._model = SklearnClassifier(Pipeline([('ml_model',model(**kwargs))]))\n",
    "        self.train_data = []\n",
    "        self.test_data = []\n",
    "        \n",
    "    def split(self,ratio,vectorizer):\n",
    "        raw_length = len(self._dataset.raw_data)\n",
    "        middle = int(raw_length/2)\n",
    "        traning_number = int(ratio*raw_length/2)\n",
    "        for item in self._dataset.preprocessed[:traning_number] + self._dataset.preprocessed[middle:middle+traning_number:]:\n",
    "            self.train_data.append((vectorizer.encode(item[self._dataset.index]),item[-1]))\n",
    "        for item in self._dataset.preprocessed[traning_number:middle] + self._dataset.preprocessed[middle+traning_number:]:\n",
    "            self.test_data.append((vectorizer.encode(item[self._dataset.index]),item[-1]))\n",
    "    \n",
    "    def train(self,train_data):\n",
    "        return self._model.train(train_data)\n",
    "    \n",
    "    def predict(self,review,vectorizer = BinaryEncoder(),tokenizer = SimpleTokenizer()):\n",
    "        return self._model.classify(vectorizer.encode(tokenizer.apply(review)))\n",
    "    \n",
    "    def load(self,path):\n",
    "        with open(path,'rb') as f:\n",
    "            self._model = pickle.load(f)\n",
    "    \n",
    "    def save(self,path):\n",
    "        with open(path,'rb') as f:\n",
    "            pickle.dump(self._model,f)\n",
    "            \n",
    "    def predict_many(self,reviews):\n",
    "        return self._model.classify_many(map(lambda t:t[0],reviews))\n",
    "    \n",
    "    def cross_validation(self,folds):\n",
    "        shuffle(self.train_data)\n",
    "        self.cv_ = []\n",
    "        fold = int(len(self.train_data)/folds)\n",
    "        for idx in range(0,len(self.train_data),fold):\n",
    "            clf = self.train(self.train_data[:idx] + self.train_data[fold+idx:])\n",
    "            y_predicted = self.predict_many(self.train_data[idx:idx +fold])\n",
    "            a = accuracy_score(list(map(lambda d:d[1],self.train_data[idx:idx+fold])),y_predicted)\n",
    "            (p,r,f,_) = precision_recall_fscore_support(list(map(lambda d : d[1], self.train_data[idx:idx+fold])), y_predicted, average ='macro')\n",
    "            print(p,r,f)\n",
    "            self.cv_.append((a,p,r,f))\n",
    "        self.cv_ = (np.mean(np.array(self.cv_),axis =  0))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = DataSet('amazon_reviews.txt',mapping = {'__label1__':'FAKE','__label2__':'NOT FAKE'},transformators= [SimpleTokenizer()])\n",
    "reader.transform(['DOC_ID','REVIEW_TEXT','LABEL'],apply_to = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Untr0nix\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(Pipeline(memory=None,\n",
       "         steps=[('ml_model',\n",
       "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=1000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=None,\n",
       "                           tol=0.0001, verbose=0))],\n",
       "         verbose=False))>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Classification(reader)\n",
    "clf.split(0.8,BinaryEncoder())\n",
    "clf.train(clf.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6087931951089846 0.6087979765901461 0.6087953700221765\n",
      "0.6356770274285568 0.6352553345880859 0.6348798514900102\n",
      "0.6303119118347354 0.6303623507927709 0.6302993771348202\n",
      "0.6354958909703435 0.635526636751967 0.6355076574021554\n",
      "0.6299019607843137 0.629796654176044 0.6296984034960644\n",
      "0.6024791325022272 0.6023809523809525 0.602285694844712\n",
      "0.6164687414687415 0.6161417657768022 0.6153535353535353\n",
      "0.6312948340158097 0.631608783864423 0.6312077562975233\n",
      "0.6320061847700038 0.6316917702567826 0.6316730523627077\n",
      "0.619773733398918 0.6185093060180856 0.6182905982905984\n"
     ]
    }
   ],
   "source": [
    "clf.cross_validation(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62416667, 0.62422026, 0.62400715, 0.62379913])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Lemmatization and removing all non important word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = DataSet('amazon_reviews.txt',mapping = {'__label1__':'FAKE','__label2__':'NOT FAKE'},transformators= [Lemmatization(WordNetLemmatizer())])\n",
    "reader.transform(['DOC_ID','REVIEW_TEXT','LABEL'],apply_to = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Untr0nix\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6356968978216091 0.6355791908563135 0.6355650202534768\n",
      "0.605407583775255 0.6049230381060244 0.6046903098658132\n",
      "0.592062367115521 0.5924313062486124 0.5913863818151907\n",
      "0.6173167701863354 0.617119070700886 0.6170338130100871\n",
      "0.6056015353073678 0.6051103368176538 0.605011372047068\n",
      "0.6184291367687393 0.6182987119013063 0.618267222160976\n",
      "0.5757824389316821 0.575732492743106 0.5755614021100748\n",
      "0.591252108045494 0.5911899722523279 0.5904669040114288\n",
      "0.6067826086956521 0.606597826518291 0.6063957500608759\n",
      "0.5907653905419312 0.5906597151230151 0.5906915426348769\n"
     ]
    }
   ],
   "source": [
    "clf_1 = Classification(reader)\n",
    "clf_1.split(0.8,BinaryEncoder())\n",
    "clf_1.train(clf_1.train_data)\n",
    "clf_1.cross_validation(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.60386905, 0.60390968, 0.60376417, 0.60350697])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_1.cv_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = DataSet('amazon_reviews.txt',mapping = {'__label1__':'FAKE','__label2__':'NOT FAKE'},transformators= [AdvancedLemmatization(WordNetLemmatizer())])\n",
    "reader.transform(['DOC_ID','REVIEW_TEXT','LABEL'],apply_to = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6341472461254023 0.633290998031125 0.6323148352110263\n",
      "0.6534419937850231 0.6533758639021797 0.6533940976037911\n",
      "0.624016588400317 0.6239322175151052 0.6237753991291727\n",
      "0.6486797527975194 0.6482142857142857 0.6479387389010176\n",
      "0.6674568879854184 0.6673685046877483 0.6674041601444144\n",
      "0.6286481204098442 0.6281958782614365 0.6280648672582698\n",
      "0.6520787963905363 0.651628104737271 0.6514645192795656\n",
      "0.6398346179368077 0.6400570328165918 0.6392345854004252\n",
      "0.6452840909090909 0.645165319284222 0.6440395473820268\n",
      "0.6516922386516624 0.6514300903774588 0.6514388079689573\n"
     ]
    }
   ],
   "source": [
    "clf_2 = Classification(reader,model = LinearSVC,C = 0.01)\n",
    "clf_2.split(0.8,BinaryEncoder())\n",
    "clf_2.train(clf_2.train_data)\n",
    "clf_2.cross_validation(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64416667, 0.64452803, 0.64426583, 0.64390696])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_2.cv_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DOC_ID', 'LABEL', 'RATING', 'VERIFIED_PURCHASE', 'PRODUCT_CATEGORY',\n",
       "       'PRODUCT_ID', 'PRODUCT_TITLE', 'REVIEW_TITLE', 'REVIEW_TEXT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('amazon_reviews.txt',delimiter = '\\t')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = DataSet('amazon_reviews.txt',label_column = 'RATING',transformators = [AdvancedLemmatization(WordNetLemmatizer())])\n",
    "reader.transform(['DOC_ID','REVIEW_TEXT','RATING'],apply_to = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46515737563048 0.3699837286162926 0.38123755350363986\n",
      "0.46424732449715894 0.3635583592712607 0.3755071268482704\n",
      "0.45457913591352145 0.36596005414707333 0.3755332964123859\n",
      "0.4598371427333176 0.3541408044050809 0.3678055467312412\n",
      "0.4605971016738466 0.3716027018339407 0.38141595410330076\n",
      "0.4658195189167049 0.35375705212145875 0.3616779867640376\n",
      "0.4379074351368887 0.37229046026722046 0.3756596569331819\n",
      "0.43923881626989497 0.33842346166888115 0.3456987911002357\n",
      "0.47728855033986156 0.37987982051396496 0.39270951579438984\n",
      "0.47410618354166745 0.3737015889770465 0.3850461981685275\n"
     ]
    }
   ],
   "source": [
    "clf_3 = Classification(reader,model = LinearSVC,C = 0.01)\n",
    "clf_3.split(0.8,BinaryEncoder())\n",
    "clf_3.train(clf_3.train_data)\n",
    "clf_3.cross_validation(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61375   , 0.45987786, 0.3643298 , 0.37422916])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_3.cv_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
